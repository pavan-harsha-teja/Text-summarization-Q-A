{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "colab_type": "code",
        "id": "WeptlGXN2MnF",
        "outputId": "781281df-51c3-4729-fe6c-7c8401925582"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "# nltk.download('punkt') # one time execution\n",
        "import re"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load & Read DataSet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "_5FvQ9LHGtja"
      },
      "outputs": [],
      "source": [
        "# # Read the CSV file\n",
        "# import io\n",
        "# df = pd.read_csv(io.StringIO(uploaded['tennis_articles_v4.csv'].decode(\"utf-8\")))\n",
        "df= pd.read_csv('tennis_articles_v4.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "colab_type": "code",
        "id": "hm6pa5Af4qbE",
        "outputId": "7bf41859-f291-44ae-a785-12cd486a53a1"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>article_id</th>\n",
              "      <th>article_text</th>\n",
              "      <th>source</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>Maria Sharapova has basically no friends as te...</td>\n",
              "      <td>https://www.tennisworldusa.org/tennis/news/Mar...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>BASEL, Switzerland (AP), Roger Federer advance...</td>\n",
              "      <td>http://www.tennis.com/pro-game/2018/10/copil-s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>Roger Federer has revealed that organisers of ...</td>\n",
              "      <td>https://scroll.in/field/899938/tennis-roger-fe...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>Kei Nishikori will try to end his long losing ...</td>\n",
              "      <td>http://www.tennis.com/pro-game/2018/10/nishiko...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>Federer, 37, first broke through on tour over ...</td>\n",
              "      <td>https://www.express.co.uk/sport/tennis/1036101...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   article_id                                       article_text   \n",
              "0           1  Maria Sharapova has basically no friends as te...  \\\n",
              "1           2  BASEL, Switzerland (AP), Roger Federer advance...   \n",
              "2           3  Roger Federer has revealed that organisers of ...   \n",
              "3           4  Kei Nishikori will try to end his long losing ...   \n",
              "4           5  Federer, 37, first broke through on tour over ...   \n",
              "\n",
              "                                              source  \n",
              "0  https://www.tennisworldusa.org/tennis/news/Mar...  \n",
              "1  http://www.tennis.com/pro-game/2018/10/copil-s...  \n",
              "2  https://scroll.in/field/899938/tennis-roger-fe...  \n",
              "3  http://www.tennis.com/pro-game/2018/10/nishiko...  \n",
              "4  https://www.express.co.uk/sport/tennis/1036101...  "
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Split Text into Sentences\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "eZVoc3R6G9a8"
      },
      "outputs": [],
      "source": [
        "# split the the text in the articles into sentences\n",
        "sentences = []\n",
        "for s in df['article_text']:\n",
        "  sentences.append(sent_tokenize(s))  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "V_lwimHsHB9l"
      },
      "outputs": [],
      "source": [
        "# flatten the list\n",
        "sentences = [y for x in sentences for y in x]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Text Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "iHvW8lKFHV1x"
      },
      "outputs": [],
      "source": [
        "# remove punctuations, numbers and special characters\n",
        "clean_sentences = pd.Series(sentences).str.replace(\"[^a-zA-Z]\", \" \")\n",
        "\n",
        "# make alphabets lowercase\n",
        "clean_sentences = [s.lower() for s in clean_sentences]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "colab_type": "code",
        "id": "iPBZsdeWHlHf",
        "outputId": "80775577-1018-423b-9fc5-abfe52aebb88"
      },
      "outputs": [],
      "source": [
        "# nltk.download('stopwords')# one time execution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "UKk_3HZ-Idjm"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import stopwords\n",
        "stop_words = stopwords.words('english')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "RX_NFApzIkmC"
      },
      "outputs": [],
      "source": [
        "# function to remove stopwords\n",
        "def remove_stopwords(sen):\n",
        "  sen_new = \" \".join([i for i in sen if i not in stop_words])\n",
        "  return sen_new"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "OcFap_w9Ivob"
      },
      "outputs": [],
      "source": [
        "# remove stopwords from the sentences\n",
        "clean_sentences = [remove_stopwords(r.split()) for r in clean_sentences]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 290
        },
        "colab_type": "code",
        "id": "a5Skjq6DJUtQ",
        "outputId": "eeba2eed-d0ed-42f8-f45f-dea8a99bcb5b"
      },
      "outputs": [],
      "source": [
        "# import requests\n",
        "\n",
        "# # URL of the GloVe embeddings zip file\n",
        "# url = \"http://nlp.stanford.edu/data/glove.6B.zip\"\n",
        "\n",
        "# # Send a GET request to the URL\n",
        "# response = requests.get(url)\n",
        "\n",
        "# # Save the content of the response to a file\n",
        "# with open(\"glove.6B.zip\", \"wb\") as f:\n",
        "#     f.write(response.content)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Vector Representation of Sentences\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "TsXIa7CBKsWQ"
      },
      "outputs": [],
      "source": [
        "# Extract word vectors\n",
        "word_embeddings = {}\n",
        "f = open('glove.6B/glove.6B.100d.txt', encoding='utf-8')\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    word_embeddings[word] = coefs\n",
        "f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "N3VtdSPyKxUZ"
      },
      "outputs": [],
      "source": [
        "sentence_vectors = []\n",
        "for i in clean_sentences:\n",
        "  if len(i) != 0:\n",
        "    v = sum([word_embeddings.get(w, np.zeros((100,))) for w in i.split()])/(len(i.split())+0.001)\n",
        "  else:\n",
        "    v = np.zeros((100,))\n",
        "  sentence_vectors.append(v)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "colab_type": "code",
        "id": "e3Iww3I9LYhJ",
        "outputId": "362a7dd8-b1bf-4478-ef59-34728fbbd8b2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "12718"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(sentence_vectors)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Similarity Matrix Preparation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1bh9L2pqL3gp"
      },
      "source": [
        "The next step is to find similarities among the sentences. We will use cosine similarity to find similarity between a pair of sentences. Let's create an empty similarity matrix for this task and populate it with cosine similarities of the sentences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "mm_fNZpOLxbM"
      },
      "outputs": [],
      "source": [
        "# similarity matrix\n",
        "sim_mat = np.zeros([len(sentences), len(sentences)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "oVeHkvf0MO1j"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "xTAAe-q3L4xM"
      },
      "outputs": [],
      "source": [
        "for i in range(len(sentences)):\n",
        "  for j in range(len(sentences)):\n",
        "    if i != j:\n",
        "      sim_mat[i][j] = cosine_similarity(sentence_vectors[i].reshape(1,100), sentence_vectors[j].reshape(1,100))[0,0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Applying TestRank Algorithm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "CAQUnNRWL_tA"
      },
      "outputs": [],
      "source": [
        "import networkx as nx\n",
        "\n",
        "nx_graph = nx.from_numpy_array(sim_mat)\n",
        "scores = nx.pagerank(nx_graph)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "aQCcvT3yO5Xj"
      },
      "outputs": [],
      "source": [
        "ranked_sentences = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary Extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "colab_type": "code",
        "id": "jwxtPBlgO_Gk",
        "outputId": "94f7a32b-fcd3-4295-ec49-4fb69e49342e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "When I'm on the courts or when I'm on the court playing, I'm a competitor and I want to beat every single person whether they're in the locker room or across the net.So I'm not the one to strike up a conversation about the weather and know that in the next few minutes I have to go and try to win a tennis match.\n",
            "Major players feel that a big event in late November combined with one in January before the Australian Open will mean too much tennis and too little rest.\n",
            "Speaking at the Swiss Indoors tournament where he will play in Sundays final against Romanian qualifier Marius Copil, the world number three said that given the impossibly short time frame to make a decision, he opted out of any commitment.\n",
            "\"I felt like the best weeks that I had to get to know players when I was playing were the Fed Cup weeks or the Olympic weeks, not necessarily during the tournaments.\n",
            "Currently in ninth place, Nishikori with a win could move to within 125 points of the cut for the eight-man event in London next month.\n",
            "He used his first break point to close out the first set before going up 3-0 in the second and wrapping up the win on his first match point.\n",
            "The Spaniard broke Anderson twice in the second but didn't get another chance on the South African's serve in the final set.\n",
            "\"We also had the impression that at this stage it might be better to play matches than to train.\n",
            "The competition is set to feature 18 countries in the November 18-24 finals in Madrid next year, and will replace the classic home-and-away ties played four times per year for decades.\n",
            "Federer said earlier this month in Shanghai in that his chances of playing the Davis Cup were all but non-existent.\n"
          ]
        }
      ],
      "source": [
        "# Specify number of sentences to form the summary\n",
        "sn = 10\n",
        "\n",
        "# Generate summary\n",
        "for i in range(sn):\n",
        "  print(ranked_sentences[i][1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## text summary by user input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.cluster.util import cosine_distance\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "import string\n",
        "\n",
        "# Function to read and preprocess the text\n",
        "def read_text(text):\n",
        "    sentences = sent_tokenize(text)\n",
        "    clean_sentences = [sent.lower() for sent in sentences]\n",
        "\n",
        "    # Removing stopwords and punctuation\n",
        "    stop_words = set(stopwords.words('english') + list(string.punctuation))\n",
        "    cleaned_text = []\n",
        "    for sent in clean_sentences:\n",
        "        words = word_tokenize(sent)\n",
        "        words = [word for word in words if word not in stop_words]\n",
        "        cleaned_text.append(' '.join(words))\n",
        "    return cleaned_text\n",
        "\n",
        "# Function to calculate similarity matrix\n",
        "def build_similarity_matrix(cleaned_text):\n",
        "    similarity_matrix = np.zeros((len(cleaned_text), len(cleaned_text)))\n",
        "    for i in range(len(cleaned_text)):\n",
        "        for j in range(len(cleaned_text)):\n",
        "            if i != j:\n",
        "                similarity_matrix[i][j] = sentence_similarity(cleaned_text[i], cleaned_text[j])\n",
        "    return similarity_matrix\n",
        "\n",
        "# Function to calculate similarity between sentences\n",
        "def sentence_similarity(sent1, sent2):\n",
        "    vector1 = set(sent1.split())\n",
        "    vector2 = set(sent2.split())\n",
        "    intersection = vector1.intersection(vector2)\n",
        "    return len(intersection) / (np.log(len(vector1)) + np.log(len(vector2)) + 1)\n",
        "\n",
        "\n",
        "# Function to generate summary using TextRank algorithm\n",
        "def generate_summary(text, num_sentences=3):\n",
        "    cleaned_text = read_text(text)\n",
        "    similarity_matrix = build_similarity_matrix(cleaned_text)\n",
        "\n",
        "    # Create a graph representation\n",
        "    graph = nx.from_numpy_array(similarity_matrix)\n",
        "    scores = nx.pagerank(graph)\n",
        "\n",
        "    # Sort sentences by score\n",
        "    ranked_sentences = sorted(((scores[i], sent) for i, sent in enumerate(cleaned_text)), reverse=True)\n",
        "\n",
        "    # Get top sentences\n",
        "    top_sentences = [sent for score, sent in ranked_sentences[:num_sentences]]\n",
        "\n",
        "    return ' '.join(top_sentences)\n",
        "\n",
        "# Example usage\n",
        "text = \"\"\"This project focuses on automating the task of text summarization using machine \n",
        "learning, specifically employing the TextRank algorithm. Text summarization involves \n",
        "condensing lengthy pieces of text while retaining essential information, and automation \n",
        "using machine learning is gaining traction in various applications, including text \n",
        "classification, question answering, legal text synthesis, news synthesis, and headline \n",
        "generation. \"\"\"\n",
        "summary = generate_summary(text)\n",
        "print(summary)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "include_colab_link": true,
      "name": "TestRank_Text_Summarization.ipynb",
      "provenance": [],
      "version": "0.3.2"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
